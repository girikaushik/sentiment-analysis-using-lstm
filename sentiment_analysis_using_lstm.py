# -*- coding: utf-8 -*-
"""Sentiment Analysis using LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jD0iULb5KK0OUhxGXheArENEBn0bOY-4
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from google.colab import drive
drive.mount('/content/drive')

dataset = pd.read_csv('/content/drive/MyDrive//Colab Notebooks/tweets/Tweets (1).csv')
dataset = dataset.sample(frac=1).reset_index(drop=True)               #returns fraction of the original path
dataset.head()

dataset.shape

dataset = dataset[['airline_sentiment','text']]
dataset.head()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
dataset['text'].str.len().plot.hist()

dataset['airline_sentiment'].value_counts()

dataset['airline_sentiment'].value_counts().plot.bar()



dataset['text'].apply(lambda x: x.lower())
dataset['text'] = dataset['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\s]',"",x))
dataset['text'].head()

tokenizer = Tokenizer(num_words=5000,split=" ")
tokenizer.fit_on_texts(dataset['text'].values)

X=tokenizer.texts_to_sequences(dataset['text'].values)
X=pad_sequences(X)
X[:7]                                     # values denotes that how many times that specific word appers.

X.shape

model = Sequential()
model.add(Embedding(5000,256,input_length = X.shape[1]))
model.add(Dropout(0.3))
model.add(LSTM(256,return_sequences=True, dropout = 0.3, recurrent_dropout=0.2))
model.add(LSTM(256, dropout = 0.3,recurrent_dropout =0.2))
model.add(Dense(3,activation ='softmax'))

model.compile(loss ='categorical_crossentropy',optimizer ='adam',metrics=['accuracy'])
model.summary()

y=pd.get_dummies(dataset['airline_sentiment']).values
[print(dataset['airline_sentiment'][i],y[i]) for i in range(0,7)]

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)

batch_size = 32
epochs =10
model.fit(X_train,y_train,epochs = epochs, batch_size =batch_size, verbose =2)

# model.save(r'C:\Users\kaushal tyagi\Desktop\LSTM\sentiment\Sentiment_Model.h5')
model.save(r'/content/drive/MyDrive/tweets/senti.h5')
# /content/drive/MyDrive/tweets/Tweets (1).csv

prediction = model.predict(X_test)
pos=0
# [print(dataset['text'][i], prediction[i], y_test[i]) for i in range(0,7)]
for i in range(0,100):
  print(dataset['text'][i], prediction[i], y_test[i])

